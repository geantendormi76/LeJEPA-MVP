import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from tqdm import tqdm
import sys
import re
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[1]))

from configs.config import cfg
from src.dataset import Galaxy10Dataset
from src.modeling.encoder import LeJEPA_Encoder

def find_best_ckpt(target_ep=450):
    ckpt_dir = cfg.CHECKPOINT_DIR
    candidates = list(ckpt_dir.glob(f"*ep{target_ep}.pth"))
    if not candidates:
        print(f"âš ï¸ æœªæ‰¾åˆ° Ep{target_ep}ï¼Œå°è¯•ä½¿ç”¨æœ€æ–°æ¨¡å‹...")
        all_ckpts = sorted(list(ckpt_dir.glob("*.pth")), key=lambda x: int(re.search(r'ep(\d+)', x.name).group(1)))
        return all_ckpts[-1]
    return candidates[0]

def run_finetune():
    print("ğŸš€ [Fine-tune] å¯åŠ¨å…¨é‡å¾®è°ƒæ¨¡å¼...")
    
    # 1. åŠ è½½é¢„è®­ç»ƒæƒé‡
    ckpt_path = find_best_ckpt(450)
    print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒåº•åº§: {ckpt_path.name}")
    
    device = cfg.DEVICE
    
    # å®šä¹‰å¾®è°ƒæ¨¡å‹ç»“æ„
    class LeJEPA_Classifier(nn.Module):
        def __init__(self, backbone_name, img_size, num_classes=10):
            super().__init__()
            # åŠ è½½ LeJEPA Encoder
            self.encoder = LeJEPA_Encoder(backbone_name, img_size)
            # åˆ†ç±»å¤´ (Linear Probe åªæ˜¯è¿™é‡Œçš„ä¸€å±‚ï¼Œä½†ç°åœ¨æˆ‘ä»¬è¦è®­ç»ƒæ‰€æœ‰å±‚)
            self.head = nn.Linear(self.encoder.backbone.num_features, num_classes)
            
        def forward(self, x):
            # è·å– Embedding (384ç»´)
            embedding, _ = self.encoder(x)
            # åˆ†ç±»
            return self.head(embedding)

    model = LeJEPA_Classifier(cfg.BACKBONE, cfg.IMG_SIZE).to(device)
    
    # åŠ è½½æƒé‡ (è¿‡æ»¤æ‰ projectorï¼Œåªä¿ç•™ backbone)
    state_dict = torch.load(ckpt_path, map_location=device)
    clean_state_dict = {}
    for k, v in state_dict.items():
        k = k.replace("_orig_mod.", "")
        if k.startswith("backbone."):
            # è¿™é‡Œçš„ key æ˜¯ backbone.xxxï¼Œæ­£å¥½åŒ¹é… model.encoder.backbone.xxx
            # ä½†æˆ‘ä»¬éœ€è¦æŠŠå‰ç¼€ 'backbone.' æ›¿æ¢ä¸º 'encoder.backbone.'
            clean_state_dict[f"encoder.{k}"] = v
            
    msg = model.load_state_dict(clean_state_dict, strict=False)
    print(f"âœ… æƒé‡åŠ è½½æŠ¥å‘Š: {msg}")
    
    # 2. æ•°æ®å‡†å¤‡ (æ ‡å‡†ç›‘ç£å­¦ä¹ å¢å¼º)
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(cfg.IMG_SIZE, scale=(0.8, 1.0)), # å¾®è°ƒæ—¶è£å‰ªæ¯”ä¾‹å¤§ä¸€ç‚¹
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    val_transform = transforms.Compose([
        transforms.Resize(cfg.IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    full_dataset = Galaxy10Dataset(cfg.DATA_PATH, transform=None)
    train_size = int(0.8 * len(full_dataset))
    test_size = len(full_dataset) - train_size
    
    # è¿™é‡Œçš„ Dataset éœ€è¦ä¸€ç‚¹ Hackï¼Œå› ä¸ºæˆ‘ä»¬æƒ³å¯¹ Train/Val ç”¨ä¸åŒçš„ Transform
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨ Collate æˆ–è€… Dataset å†…éƒ¨å¤„ç†ï¼Œæˆ–è€…è¿™é‡Œç›´æ¥ç”¨ä¸¤ä¸ª Dataset å¯¹è±¡
    train_set = Galaxy10Dataset(cfg.DATA_PATH, transform=train_transform)
    val_set = Galaxy10Dataset(cfg.DATA_PATH, transform=val_transform)
    
    # ä½¿ç”¨ indices åˆ’åˆ†
    indices = torch.randperm(len(full_dataset)).tolist()
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    train_loader = DataLoader(torch.utils.data.Subset(train_set, train_indices), 
                            batch_size=64, shuffle=True, num_workers=4)
    val_loader = DataLoader(torch.utils.data.Subset(val_set, val_indices), 
                          batch_size=64, shuffle=False, num_workers=4)

    # 3. å·®åˆ†å­¦ä¹ ç‡ç­–ç•¥ (Differential Learning Rates) - SOTA ç§˜è¯€
    # Backbone ç”¨å°ç«æ…¢ç‚– (é˜²æ­¢ç ´åé¢„è®­ç»ƒç‰¹å¾)ï¼ŒHead ç”¨å¤§ç«çˆ†ç‚’
    optimizer = optim.AdamW([
        {'params': model.encoder.parameters(), 'lr': 1e-4}, # é¢„è®­ç»ƒå±‚: 1e-4
        {'params': model.head.parameters(), 'lr': 1e-3}     # æ–°å±‚: 1e-3
    ], weight_decay=0.05)
    
    criterion = nn.CrossEntropyLoss()
    
    # 4. è®­ç»ƒå¾ªç¯ (å¾®è°ƒåªéœ€è¦è·‘ 20-30 è½®)
    best_acc = 0.0
    
    for epoch in range(30):
        model.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"Finetune Ep {epoch+1}/30")
        
        for imgs, labels in pbar:
            imgs, labels = imgs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            logits = model(imgs)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})
            
        # Eval
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                logits = model(imgs)
                _, predicted = torch.max(logits, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        acc = 100 * correct / total
        print(f"Epoch {epoch+1}: Val Acc = {acc:.2f}%")
        
        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), "best_finetuned_model.pth")
            print(f"ğŸ”¥ New Best! Saved.")

    print(f"âœ¨ å¾®è°ƒå®Œæˆï¼æœ€ç»ˆæœ€ä½³ç²¾åº¦: {best_acc:.2f}%")

if __name__ == "__main__":
    run_finetune()